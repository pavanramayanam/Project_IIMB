# -*- coding: utf-8 -*-
"""scrapping.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10XcFMRYJaLtc6uQXDdWmupH1z9iWvI-y
"""

import requests
import json
from bs4 import BeautifulSoup
import os
import pandas as pd
from constants import MYNTRA_DATA_FILE, PRODUCT_CSV_FILE, IMAGES_DIR

headers = {'User-Agent': 'Chrome/84.0.4147.89'}

def scrape_product_data(url):
    s = requests.Session()
    res = s.get(url, headers=headers, verify=False)

    soup = BeautifulSoup(res.text, "html.parser")

    script = None
    for s in soup.find_all("script"):
        if 'pdpData' in s.text:
            script = s.get_text(strip=True)
            break

    product_data = json.loads(script[script.index('{'):])
    return product_data['pdpData']

def download_images(product_data, img_directory=IMAGES_DIR):
    save_directory = os.path.join(img_directory, str(product_data['id']))
    os.makedirs(save_directory, exist_ok=True)

    image_paths = []  # Store image paths in a list

    for image in product_data['media']['albums'][0]['images']:
        image_url = image['src']
        image_name = image_url.split('/')[-1]
        image_path = os.path.join(save_directory, image_name)

        response = requests.get(image_url, stream=True)
        with open(image_path, 'wb') as image_file:
            for chunk in response.iter_content(chunk_size=128):
                image_file.write(chunk)

        print(f"Downloaded: {image_path}")

        image_paths.append(image_path)  # Append image path to the list

    product_data['image_paths'] = image_paths  # Add image paths to product data

    return product_data

if __name__ == "__main__":
    # Check if the CSV file exists
    if os.path.exists(MYNTRA_DATA_FILE):
        existing_products = set(pd.read_csv(MYNTRA_DATA_FILE)['id'])
    else:
        existing_products = set()

    # Read product URLs from CSV and filter based on existing product IDs
    df = pd.read_csv(PRODUCT_CSV_FILE)
    product_urls = df[df['Product_id'].isin(existing_products) == False]['URL'].sample(50).values.tolist()

    data_list = []  # List to store data for DataFrame

    for url in product_urls:
        product_data = scrape_product_data(url)

        product_data = download_images(product_data)

        # Extract required information
        data = {
            'id': product_data['id'],
            'name': product_data['name'],
            'product_details_description': [item['description'] for item in product_data['productDetails'] if item['title'] == 'Product Details'][0],
            **product_data['analytics'],
            'image_paths': product_data['image_paths']
        }

        data_list.append(data)

    # Create DataFrame
    result_df = pd.DataFrame(data_list)

    # Save DataFrame to CSV (append if it already exists)
    mode = 'a' if os.path.exists(MYNTRA_DATA_FILE) else 'w'
    result_df.drop(columns='colourHexCode').to_csv(MYNTRA_DATA_FILE, mode=mode, index=False, header=not os.path.exists(MYNTRA_DATA_FILE))
    print("Saved DataFrame to CSV.")